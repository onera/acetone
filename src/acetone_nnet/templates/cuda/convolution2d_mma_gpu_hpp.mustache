
template<typename F, size_t M, size_t N, size_t K>
__global__ void wmma_kernel(const F* a, const F* b, const F *c, F *d) {
    using namespace nvcuda;

    __shared__ __half ah[M * K];
    __shared__ __half bh[K * N];

    for (size_t i = 0; i < M * K; ++i)
    {
        ah[i] = __half(a[i]);
    }

    for (size_t i = 0; i < K * N; ++i)
    {
        bh[i] = __half(b[i]);
    }

//    for (size_t m = 0; m < M; ++m)
//    {
//        for (size_t n = 0; n < N; ++n)
//        {
//            d[m * N + n] = c[m * N + n];
//            for (size_t k = 0; k < K; ++k)
//            {
//                d[m * N + n] += float(ha[m * K + k] * hb[k * N + n]);
//            }
//        }
//    }
//    return;

    // Declare the fragments
    wmma::fragment<wmma::matrix_a, M, N, K, __half, wmma::row_major> a_frag;
    wmma::fragment<wmma::matrix_b, M, N, K, __half, wmma::row_major> b_frag;
    wmma::fragment<wmma::accumulator, M, N, K, float> c_frag;
    wmma::fragment<wmma::accumulator, M, N, K, float> d_frag;

    // Initialize the output to zero
    wmma::fill_fragment(d_frag, 0.0f);

    // Load the inputs and accumulator
    wmma::load_matrix_sync(a_frag, ah, K);
    wmma::load_matrix_sync(b_frag, bh, N);
    wmma::load_matrix_sync(c_frag, const_cast<float*>(c), N, wmma::mem_row_major);

    // Perform the matrix multiplication
    wmma::mma_sync(d_frag, a_frag, b_frag, c_frag);

    // Store the output
    wmma::store_matrix_sync(d, d_frag, N, wmma::mem_row_major);


}

/* Perform a matrix-multiply add operation: D = A * B + C */
/* TODO Specialise for "accepted" variants on platform. */
/* TODO Assume the input matrices have been CUDA memory-mapped */
template<typename F, size_t M, size_t N, size_t K>
void perform_mma(const F *a, const F *b, F *c, F *d)
{
    F *da;
    F *db;
    F *dc;
    F *dd;

    cuda_check_errors(cudaMalloc(reinterpret_cast<void **>(&da), M * K * sizeof(F)));
    cuda_check_errors(cudaMalloc(reinterpret_cast<void **>(&db), K * N * sizeof(F)));
    cuda_check_errors(cudaMalloc(reinterpret_cast<void **>(&dc), M * N * sizeof(F)));
    cuda_check_errors(cudaMalloc(reinterpret_cast<void **>(&dd), M * N * sizeof(F)));

    cuda_check_errors(cudaMemcpy(da, a, M * K * sizeof(F), cudaMemcpyHostToDevice));
    cuda_check_errors(cudaMemcpy(db, b, K * N * sizeof(F), cudaMemcpyHostToDevice));
    cuda_check_errors(cudaMemcpy(dc, c, M * N * sizeof(F), cudaMemcpyHostToDevice));
    cuda_check_errors(cudaMemcpy(dd, d, M * N * sizeof(F), cudaMemcpyHostToDevice));

    wmma_kernel<F, M, N, K><<<1, 32>>>(da, db, dc, dd);
    cudaError_t err = cudaGetLastError();
    if (err != cudaSuccess)
    {
        printf("Error: %s\n", cudaGetErrorString(err));
    }

    cuda_check_errors(cudaMemcpy(d, dd, M * N * sizeof(F), cudaMemcpyDeviceToHost));
}

template <typename F>
int Conv2D(int layer_idx, F *input, F *output)
{
    // Number of filters
    const size_t FF = net[layer_idx].nb_filters;
    // Number of channels
    const size_t CC = net[layer_idx].input_channels;
    // Output spatial dimensions
    const size_t OH = net[layer_idx].output_height;
    const size_t OW = net[layer_idx].output_width;
    // Kernel spatial dimensions
    const size_t KH = net[layer_idx].kernel_size;
    const size_t KW = net[layer_idx].kernel_size;
    // Input spatial dimensions
    const size_t IH = net[layer_idx].input_height;
    const size_t IW = net[layer_idx].input_width;
    // Convolution parameters
    const size_t pad_left = net[layer_idx].pad_left;
    const size_t pad_top = net[layer_idx].pad_top;
    const size_t strides = net[layer_idx].strides;
    const size_t dilation = net[layer_idx].dilation_rate;

    // MMA dimensions
    const size_t M = 32;
    const size_t N = 8;
    const size_t K = 16;

    // Initialise output with biases
    // for i in range(volume_of((OH, OW, FF))):
        // oh, ow, g = indices_of(i, (OH, OW, FF))
        // output[oh, ow, g] = biases[g]
    for (size_t i = 0, ilen = OH * OW * FF; i < ilen; ++i)
    {
        output[i] = net[layer_idx].biases[i % FF];
    }

    // Process filters M at a time (mapping each filter to a line in A)
    // for g in range(0, FF, M):
    for (size_t g = 0; g < FF; g += M)
    {
        // Process output elements N at time (mapping each output element to a column in B)
        // for i in range(0, volume_of((OH, OW)), N):
        for (size_t i = 0, ilen=OH * OW; i < ilen; i += N)
        {
            // Compute output[i:i+n,g] using K fragments at a time
            // for k in range(0, volume_of((KH, KW, CC)), K):
            for (size_t k = 0, klen=KH * KW * CC; k < klen; k += K)
            {
                // A = np.zeros((M, K))
                // B = np.zeros((K, N))

                // Fragments of weights, input, and output matrices
                F *A;
                F *B;
                F *X;
                F *D;

                cuda_check_errors(cudaMallocHost(&A, M * K * sizeof(F)));
                cuda_check_errors(cudaMallocHost(&B, K * N * sizeof(F)));
                cuda_check_errors(cudaMallocHost(&X, M * N * sizeof(F)));
                cuda_check_errors(cudaMallocHost(&D, M * N * sizeof(F)));

                for (size_t m = 0; m < M; ++m)
                {
                    for (size_t n = 0; n < N; ++n)
                    {
                        X[m * N + n] = 0.0;
                        D[m * N + n] = 0.0;
                    }
                }

                // Copy weights[k:k+K, g:g+M] into A[0:M, 0:K]
                // TODO Add a as a loop index
                for (size_t ah = 0; ah < M; ++ah)
                {
                    for (size_t aw = 0; aw < K; ++aw)
                    {
                        // A[:M, :] identifies a filter in weights[:,g:g+M]
                        // A[:,  :K] identifies the contents of a filter in weights[k:k+K,:]
                        // if g + ah < FF and k + aw < volume_of((KH, KW, CC)):
                        if (g + ah < FF && k + aw < klen)
                        {
                            // kh, kw, c = indices_of(k + aw, (KH, KW, CC))
                            size_t kh = (k + aw) % (KH * KW * CC) / (KW * CC);
                            size_t kw = (k + aw) % (KW * CC) / CC;
                            size_t kc = (k + aw) % (CC) / 1;
                            // TODO Check weights[kh, kw, kc,:] is weights[k,:] and simplify
                            // A[ah, aw] = weights[kh, kw, c, g + ah]
                            A[ah * K + aw] = net[layer_idx].weights[((kh * KW + kw) * CC + kc) * FF + (g + ah)];
                        }
                        else
                        {
                            A[ah * K + aw] = 0.0;
                        }

                    }
                }

                // Copy matching input into B
                // for b in range(volume_of((K, N))):
                for (size_t bh = 0; bh < K; ++bh)
                {
                    for (size_t bw = 0; bw < N; ++bw)
                    {
                        // bh, bw = indices_of(b, (K, N))


                        // if i + bw < volume_of((OH, OW)) and k + bh < volume_of((KH, KW, CC)):
                        if (i + bw < ilen && k + bh < klen)
                        {
                            // oh, ow = indices_of(i + bw, (OH, OW))
                            size_t oh = (i + bw) % (OH * OW) / OW;
                            size_t ow = (i + bw) % OW / 1;
                            // kh, kw, c = indices_of(k + bh, (KH, KW, C))
                            size_t kh = (k + bh) % (KH * KW * CC) / (KW * CC);
                            size_t kw = (k + bh) % (KW * CC) / CC;
                            size_t kc = (k + bh) % (CC) / 1;
                            // Compute input indices
                            // ih = input_index_of(oh, kh, strides, dilation, pad_left)
                            size_t ih = oh * strides + kh * dilation - pad_left;
                            // iw = input_index_of(ow, kw, strides, dilation, pad_top)
                            size_t iw = ow * strides + kw * dilation - pad_top;
                            if (0 <= ih < IH && 0 <= iw < IW)
                            {
                                // B[bh, bw] = input[ih, iw, c]
                                B[bh * N + bw] = input[(ih * IW + iw) * CC + kc];
                            }
                            else
                            {
                                B[bh * N + bw] = 0.0;
                            }

                        }
                        else
                        {
                            B[bh * N + bw] = 0.0;
                        }
                    }
                }

                // D = mma(M, N, K, A, B, np.zeros((M, N)))
                perform_mma<F, M, N, K>(A, B, X, D);

                // Copy result into output
                // for m in range(M):
                for (size_t m = 0; m < M; ++m)
                {
                    // for n in range(N):
                    for (size_t n = 0; n < N; ++n)
                    {
                        // Each line in D is a filter
                        // Each column in D is an output element
                        // if g + m < FF and i + n < volume_of((OH, OW)):
                        if (g + m < FF && i + n < ilen)
                        {
                            // oh, ow = indices_of(i + n, (OH, OW))
                            size_t oh = (i + n) % (OH * OW) / OW;
                            size_t ow = (i + n) % OW / 1;
                            // output[oh, ow, g + m] += D[m, n]
                            output[(oh * OW + ow) * FF + (g + m)] += D[m * N + n];
                        }
                    }
                }

                cuda_check_errors(cudaFreeHost(A));
                cuda_check_errors(cudaFreeHost(B));
                cuda_check_errors(cudaFreeHost(X));
                cuda_check_errors(cudaFreeHost(D));
            }
        }
    }

    // Apply activation function
    for (size_t i = 0, ilen = OH * OW * FF; i < ilen; ++i)
    {
        output[i] = net[layer_idx].actv_function(output[i]);
    }

    return 0;
}
