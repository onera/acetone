#include "inference.h"

#define MAX_BATCH_SIZE {{max_batch_size}}
/* Activation and temp tensor allocation */
inference_t Context[MAX_BATCH_SIZE];

